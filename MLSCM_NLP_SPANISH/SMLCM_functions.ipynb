{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0e6443d",
   "metadata": {},
   "source": [
    "## Functions: Models and cleaning data\n",
    "\n",
    "### Prepare and clean the data\n",
    "\n",
    "\n",
    "Function to vectorize the corpus\n",
    "\n",
    "Two tipes of vectorization process\n",
    "\n",
    "     - vectorizer1: matrix counting words. \n",
    "\n",
    "     - vectorizer2: matrix  counting words but penalizing repeated words (IDF)\n",
    "\n",
    "Clean data\n",
    "\n",
    "    - Tildes: It takes the string and change a character with a tilde for a character without it.\n",
    "    -preprocessor: It takes all the emoticons apart from the aphabetic characters.\n",
    "    -tokenize: Split the text using whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459cb96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basics\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from joblib import dump, load\n",
    "#import text_cleaning  # Module in directory. \n",
    "## Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "## Pipelines/Model Evaluation\n",
    "from sklearn.model_selection import GridSearchCV #\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "import matplotlib.pyplot as plt\n",
    "## Classifiers\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bc61ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_es= stopwords.words(\"spanish\")\n",
    "\n",
    "vectorizer1 = CountVectorizer(lowercase=True, preprocessor=None)\n",
    "vectorizer2 = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "\n",
    "def myTokenizer(sentence):\n",
    "        newSent = []\n",
    "        for word in list(word_tokenize(sentence)):\n",
    "            if len(word)>1 or word[0].isalpha():\n",
    "                newSent.append(word)\n",
    "        return newSent\n",
    "\n",
    "\n",
    "snowballStemmer = SnowballStemmer('spanish')\n",
    "# Defining language for snowball class\n",
    "\n",
    "def tokenize_snowball(text):\n",
    "    return [snowballStemmer.stem(word) for word in text.split()]\n",
    "# Stem each word\n",
    "\n",
    "\n",
    "def tildes(texto):\n",
    "\t'''\n",
    "\teliminates the tildes (especial problematic in spanish), e.g = from é to e\n",
    "\timput= text\n",
    "\toutput= text without tildes\n",
    "\t'''\n",
    "\tdictionary = {'á':'a','é':'e','í':'i','ó':'o','ú':'u'}\n",
    "\ttemp=texto\n",
    "\tfor i in list(dictionary):\n",
    "\t\ttemp=re.sub(i,dictionary.pop(i),temp)\n",
    "\treturn(temp)\n",
    "\n",
    "def preprocessor(text):\n",
    "    text = re.sub(\"<[^>]*>\", \"\", text)\n",
    "    emoticons = re.findall(\"(?::|;|=)(?:-)?(?:\\)|\\(|D|P)\", text)\n",
    "    text = re.sub(\"[\\W]+\", \" \", text.lower()) + \" \" + \" \".join(emoticons).replace(\"-\", \"\")\n",
    "    return(text)\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5898f5fe",
   "metadata": {},
   "source": [
    "## Models \n",
    "\n",
    "I create a pipeline for different classification models. Using different hyperparameters.\n",
    "The parameter grid has multiple sets of parameters for vectorizing and the ML model.\n",
    "The aim is to compare the results from different parameters's configuration. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c27b62",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2828f1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the models that will be used\n",
    "pip1 = Pipeline([(\"vect\", vectorizer1),\n",
    "                # Countvectorizer : Tokenize strings\n",
    "                (\"clf\", BernoulliNB(alpha=.01))])\n",
    "                # BernoulliNB : Model for binary data\n",
    "\n",
    "# Defining sets of parameters for the models\n",
    "param_grid_nb = [{  \"vect__ngram_range\": [(1,1),(1,2)],\n",
    "                    # Number of contiguous sequences of words in a setence\n",
    "                    # (1,1) : Try only unigrams\n",
    "                    # (1,2) : unigrams and bigrams\n",
    "                    \"vect__stop_words\": [stop_es, None],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary\n",
    "                    # None : No words removed\n",
    "                    \"vect__tokenizer\": [None,myTokenizer,tokenize_snowball],\n",
    "                    # None : Override tokenization\n",
    "                    # myTokenizer : Use the function myTokenizer\n",
    "                    # tokenize_snowball : Use the function tokenize_snowball\n",
    "                    \"vect__binary\":[True], \n",
    "                    # Non zero counts of words or tokens are set to 1\n",
    "                    # the word exists in the document or not\n",
    "                    \"clf__fit_prior\":[True,False]\n",
    "                    # True : Use previous probabilities\n",
    "                    # False : Do not use previous probabilities\n",
    "                }\n",
    "                ]\n",
    "\n",
    "# Estimate the model with different hyperparameters\n",
    "nby1 = GridSearchCV(pip1, \n",
    "                    # Order Functions and models to execute\n",
    "                    param_grid_nb , \n",
    "                    # Parameters that will change to contrast results\n",
    "                    scoring = \"accuracy\", \n",
    "                    # Output to compare\n",
    "                    cv = 3, \n",
    "                    # Number of folds for cross validation\n",
    "                    verbose = 1, \n",
    "                    # Request more information  about the model\n",
    "                    n_jobs = 4) # Number of processes executed parallelly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a840ef0",
   "metadata": {},
   "source": [
    "### Logit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff64ff88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the models that will be used\n",
    "pip2 = Pipeline([(\"vect\", vectorizer2),\n",
    "                # TfidfVectorizer : Tokenize strings\n",
    "                (\"clf\", LogisticRegression(random_state=42))])\n",
    "                # LogisticRegression : Model for binary feature\n",
    "\n",
    "# Defining sets of parameters for the models\n",
    "param_grid_log = [{ ### First group of parameters\n",
    "                    \n",
    "                    ## Parameters of TfidfVectorizer\n",
    "                    \"vect__ngram_range\": [(1,1),(1,2)],\n",
    "                    # Number of contiguous sequences of words in a setence\n",
    "                    # (1,1) : Try only unigrams\n",
    "                    # (1,2) : unigrams and bigrams\n",
    "                    \"vect__stop_words\": [stop_es, None],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                    \"vect__tokenizer\": [None,myTokenizer,\n",
    "                                  tokenize_snowball],\n",
    "                    # None : Override tokenization\n",
    "                    # myTokenizer : Use the function myTokenizer\n",
    "                    # tokenize_snowball : Use the function tokenize_snowball\n",
    "\n",
    "                    ## Hyperparamters for LogisticRegression\n",
    "                    \"clf__penalty\": [\"l2\"],\n",
    "                    # l2 : Type of penalty\n",
    "                    \"clf__C\": [8.0, 12.0]\n",
    "                    # Regularization strength\n",
    "                    # Smaller values stronger regularization\n",
    "                }, \n",
    "                    ### Second group of parameters\n",
    "\n",
    "                    ## Parameters of TfidfVectorizer\n",
    "                {   \"vect__ngram_range\": [(1,1),(1,2)],\n",
    "                    # Number of contiguous sequences of words in a setence\n",
    "                    # Try only unigrams and unigrams and bigrams\n",
    "                    \"vect__stop_words\": [stop_es, None],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                    \"vect__tokenizer\": [None,myTokenizer,\n",
    "                                  tokenize_snowball],\n",
    "                    # None : Override tokenization\n",
    "                    # myTokenizer : Use the function myTokenizer\n",
    "                    # tokenize_snowball : Use the function tokenize_snowball\n",
    "                    \"vect__use_idf\": [False], \n",
    "                    # False : Enable inverse-document-frequency reweighting\n",
    "                    # The previous paremeter by default is True\n",
    "                    \"vect__norm\": [None],\n",
    "                    # It will not normalize the tfidf matrix\n",
    "                    \n",
    "                    ## Hyperparamters for LogisticRegression\n",
    "                    \"clf__penalty\": [\"l1\",\"l2\"],\n",
    "                    # l1 : The norm used in penalization\n",
    "                    # l2 : The norm used in penalization\n",
    "                    \"clf__C\": [1.0, 10.0]}\n",
    "                    # Regularization strength\n",
    "                    # Smaller values stronger regularization\n",
    "             ]\n",
    "\n",
    "# Estimate the model with different hyperparameters\n",
    "log1 = GridSearchCV(pip2, \n",
    "                    # Order Functions and models to execute\n",
    "                    param_grid_log,\n",
    "                    # Parameters that will change to contrast results\n",
    "                    scoring=\"recall\",\n",
    "                    # Report  the ratio of true positives samples divide by \n",
    "                    # The sum of true positive samples and false negatives\n",
    "                    cv=3,\n",
    "                    # Number of folds for cross validation\n",
    "                    verbose=1,\n",
    "                    # Request more information  about the model\n",
    "                    n_jobs=4)# Number of processes executed parallelly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c1f3fd",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac1a5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SVC \n",
    "# Defining the models that will be used\n",
    "pip3 = Pipeline([(  \"vect\", vectorizer2),\n",
    "                    # TfidfVectorizer : Tokenize strings\n",
    "                    (\"clf\", SVC(random_state=42))])\n",
    "                    # SVC : Support Vector Classification.\n",
    "\n",
    "# Defining sets of parameters for the models\n",
    "param_grid_svc = [{## Parameters of TfidfVectorizer\n",
    "                    \"vect__ngram_range\": [(1,1),(1,2)],\n",
    "                    # Number of contiguous sequences of words in a setence\n",
    "                    # (1,1) : Try only unigrams\n",
    "                    # (1,2) : unigrams and bigrams\n",
    "                    \"vect__stop_words\": [stop_es, None],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                    \"vect__tokenizer\": [None,myTokenizer,\n",
    "                                  tokenize_snowball],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "\n",
    "                    ## SVC hyperparameters\n",
    "                    \"clf__kernel\": [\"linear\",\"rbf\"],\n",
    "                    # Linear : single line\n",
    "                    # rbf : Radial basis function kernel,\n",
    "                    \"clf__C\": [.1, 1.0, 10.0],\n",
    "                    # Regularization strength\n",
    "                    # Smaller values stronger regularization\n",
    "                    \"clf__gamma\": [.1, 1.0, 10.0,'auto']}]\n",
    "                    # Kernel coefficient for rbf\n",
    "                    # auto : 1/n_features\n",
    "\n",
    "# Estimate the model with different hyperparameters\n",
    "svc=GridSearchCV(pip3,\n",
    "                # Order Functions and models to execute\n",
    "                param_grid_svc,\n",
    "                # Parameters that will change to contrast results\n",
    "                scoring=\"accuracy\",\n",
    "                # Report  accuracy results\n",
    "                cv=3, \n",
    "                # Number of folds for cross validation\n",
    "                verbose=1,\n",
    "                # Request more information  about the model\n",
    "                n_jobs=4)# Number of processes executed parallelly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a89473",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ab42b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Forest\n",
    "# Defining the models that will be used\n",
    "pip4 = Pipeline([(  \"vect\", vectorizer1),\n",
    "                    # Countvectorizer : Tokenize strings\n",
    "                    (\"clf\", RandomForestClassifier(random_state=0))])\n",
    "                    # Random Forest Model \n",
    "\n",
    "# Defining sets of parameters for the models\n",
    "param_grid_rf = [{\"vect__ngram_range\": [(1,1),(1,2)],\n",
    "                    # Number of contiguous sequences of words in a setence\n",
    "                    # (1,1) : Try only unigrams\n",
    "                    # (1,2) : unigrams and bigrams\n",
    "                    \"vect__stop_words\": [stop_es, None],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                    \"vect__tokenizer\": [None,myTokenizer,\n",
    "                                  tokenize_snowball],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "\n",
    "                    # Model hyperparameters\n",
    "                    \"clf__max_depth\": [None,5,10,20],\n",
    "                    # Number of nodes\n",
    "                    \"clf__max_features\": [None,'log2','sqrt'],\n",
    "                    # The number of features to consider when looking for the best split\n",
    "                    \"clf__n_estimators\": [10,20,30,100]}]\n",
    "                    # Number of trees in the forest\n",
    "\n",
    "# Estimate the model with different hyperparameters\n",
    "rf1 = GridSearchCV(pip4,\n",
    "                    # Order Functions and models to execute\n",
    "                    param_grid_rf,\n",
    "                    # Parameters that will change to contrast results\n",
    "                    scoring=\"accuracy\",\n",
    "                    # Report  accuracy results\n",
    "                    cv=3, \n",
    "                    # Number of folds for cross validation\n",
    "                    verbose=1,\n",
    "                    # Request more information  about the model\n",
    "                    n_jobs=4)# Number of processes executed parallelly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c573e1",
   "metadata": {},
   "source": [
    "### K-nearest Neigbhbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e07fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the models that will be used\n",
    "pip5 = Pipeline([(\"vect\", vectorizer2),\n",
    "                    # TfidfVectorizer : Tokenize strings\n",
    "                    (\"clf\", KNeighborsClassifier())])\n",
    "                    # Model KNN\n",
    "\n",
    "# Defining sets of parameters for the models\n",
    "param_grid_KKN = [{\"vect__ngram_range\": [(1,1),(1,2)],\n",
    "                    # Number of contiguous sequences of words in a setence\n",
    "                    # (1,1) : Try only unigrams\n",
    "                    # (1,2) : unigrams and bigrams\n",
    "                    \"vect__stop_words\": [stop_es, None],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                    \"vect__tokenizer\": [None,myTokenizer,\n",
    "                                  tokenize_snowball],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "\n",
    "                    ## Model Hyperparameters\n",
    "                    \"clf__n_neighbors\": [3,5,7,10],\n",
    "                    # Number of neighbors\n",
    "                    \"clf__weights\": ['uniform','distance'],\n",
    "                    # Weight function\n",
    "                    # uniform : equal weight for each neighbor\n",
    "                    # distance : based on distance of each neighbor\n",
    "                    \"clf__p\": [1,2]}]\n",
    "                    # Type of distance\n",
    "                    # 1 : Minkowski distance\n",
    "                    # 2 : Euclidean distance\n",
    "\n",
    "# Estimate the model with different hyperparameters\n",
    "KKN = GridSearchCV(pip5, \n",
    "                    # Order Functions and models to execute\n",
    "                    param_grid_KKN,\n",
    "                    # Parameters that will change to contrast results\n",
    "                    scoring=\"accuracy\",\n",
    "                    # Report  accuracy results\n",
    "                    cv=3, \n",
    "                    # Number of folds for cross validation\n",
    "                    verbose=1,\n",
    "                    # Request more information  about the model\n",
    "                    n_jobs=4) # Number of processes executed parallelly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3ea66c",
   "metadata": {},
   "source": [
    "### Complement Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1381d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Defining the models that will be used\n",
    "pip6 = Pipeline([(\"vect\", vectorizer1),\n",
    "                    # Countvectorizer : Tokenize strings\n",
    "                    (\"clf\", ComplementNB(alpha=.01))])\n",
    "                    # Model ComplementNB\n",
    "\n",
    "# Defining sets of parameters for the models\n",
    "param_grid_cnb = [{\"vect__ngram_range\": [(1,1),(1,2)],\n",
    "                    # Number of contiguous sequences of words in a setence\n",
    "                    # (1,1) : Try only unigrams\n",
    "                    # (1,2) : unigrams and bigrams\n",
    "                    \"vect__stop_words\": [stop_es, None],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                    \"vect__tokenizer\": [None,myTokenizer,tokenize_snowball],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                   \"vect__binary\":[True],\n",
    "                    # Non zero counts of words or tokens are set to 1\n",
    "                    # the word exists in the document or not\n",
    "\n",
    "                    ## Model Hyperparameters\n",
    "                    \"clf__fit_prior\":[True,False],\n",
    "                    # True : Edge case\n",
    "                    # False : Not edge case\n",
    "                    \"clf__norm\":[True,False]}\n",
    "                    # Normalization\n",
    "             ]\n",
    "\n",
    "cnb = GridSearchCV(pip6, \n",
    "                    # Order Functions and models to execute\n",
    "                    param_grid_cnb,\n",
    "                    # Parameters that will change to contrast results\n",
    "                    scoring=\"accuracy\",\n",
    "                    # Report  accuracy results\n",
    "                    cv=3, \n",
    "                    # Number of folds for cross validation\n",
    "                    verbose=1,\n",
    "                    # Request more information  about the model\n",
    "                    n_jobs=4)# Number of processes executed parallelly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12517af1",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01ec6ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the models that will be used\n",
    "pip7 = Pipeline([(\"vect\", vectorizer2),\n",
    "                    # TfidfVectorizer : Tokenize strings\n",
    "                    (\"clf\", LinearSVC())])\n",
    "                    # Model Linear SVC\n",
    "\n",
    "# Defining sets of parameters for the models\n",
    "param_grid_Lsvc = [{\"vect__ngram_range\": [(1,1),(1,2)],\n",
    "                    # Number of contiguous sequences of words in a setence\n",
    "                    # (1,1) : Try only unigrams\n",
    "                    # (1,2) : unigrams and bigrams\n",
    "                    \"vect__stop_words\": [stop_es, None],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                    \"vect__tokenizer\": [None,myTokenizer,\n",
    "                                  tokenize_snowball],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "              \n",
    "                    ## Model Hyperparameters\n",
    "                    \"clf__C\": [.1, 1.0, 10.0],\n",
    "                    # Regularization strength\n",
    "                    # Smaller values stronger regularization\n",
    "                    \"clf__penalty\":[\"l1\",\"l2\"],\n",
    "                    # Type of penalty: l1 and l2\n",
    "                    \"clf__dual\":[False],\n",
    "                    # False : n_samples > n_features\n",
    "                    \"clf__multi_class\":[\"ovr\",\"crammer_singer\"]},\n",
    "                    # ovr : Trains n_clases one-vs-rest classifiers\n",
    "                    # crammer_singer : Joint objective over all classes\n",
    "                \n",
    "                ## Second group of parameters\n",
    "                    {\"vect__ngram_range\": [(1,1),(1,2)],\n",
    "                    # Number of contiguous sequences of words in a setence\n",
    "                    # (1,1) : Try only unigrams\n",
    "                    # (1,2) : unigrams and bigrams\n",
    "                    \"vect__stop_words\": [stop_es, None],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                    \"vect__tokenizer\": [None,myTokenizer,\n",
    "                                  tokenize_snowball],\n",
    "                    # Words that will be removed from tokens\n",
    "                    # stop_es : some specific words from spanish vocabulary presumed to be uninformative\n",
    "                    # None : No words removed\n",
    "                    \"clf__C\": [.1, 1.0, 10.0],\n",
    "                    # Regularization strength\n",
    "                    # Smaller values stronger regularization\n",
    "                    \"clf__penalty\":[\"l2\"],\n",
    "                    # Type of penalty: only l2\n",
    "                    \"clf__dual\":[True,False],\n",
    "                    # True : Dual optimization\n",
    "                    # False : Primal optimization\n",
    "                    \"clf__multi_class\":[\"ovr\",\"crammer_singer\"]\n",
    "                    # new need to be testes \n",
    "                    # ovr : Trains n_clases one-vs-rest classifiers\n",
    "                    # crammer_singer : Joint objective over all classes\n",
    "                  }]\n",
    "\n",
    "# Estimate the model with different hyperparameters\n",
    "Lsvc=GridSearchCV(  pip7,\n",
    "                    # Order Functions and models to execute\n",
    "                    param_grid_Lsvc,\n",
    "                    # Parameters that will change to contrast results\n",
    "                    scoring=\"accuracy\",\n",
    "                    # Report  accuracy results\n",
    "                    cv=3, \n",
    "                    # Number of folds for cross validation\n",
    "                    verbose=1,\n",
    "                    # Request more information  about the model\n",
    "                    n_jobs=4) # Number of processes executed parallelly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96325260",
   "metadata": {},
   "source": [
    "### Wrap up estimation\n",
    "\n",
    "We write two functions to estimate all the models at once with the same data.\n",
    "\n",
    "- estimation\n",
    "This function takes the nby1,svc,rf1,KKN,Lsvc grid objects in a list and fit them with the data.\n",
    "Then, it  stores the results in a dataframe.\n",
    "\n",
    "- estimation 2\n",
    "This function takes the nby1,Lsvc grid objects in a list and fit them with the data.\n",
    "Then, it  stores the results in a dataframe.\n",
    "\n",
    "#### Pendings\n",
    "\n",
    "Get the fpr, tpr, and thresholds and the number of fits by model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "61e10847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a Pandas DataFrame\n",
    "scores=pd.DataFrame(columns=['Country','model','accuracy',])\n",
    "\n",
    "#fits=pd.DataFrame(columns=['Country','model','fits','cv']) #not working\n",
    "\n",
    "\n",
    "\n",
    "def estimation(X_train_texto, y_train, X_test_texto, y_test, Country):\n",
    "    '''\n",
    "    Estimates: Naive bayes, Logit, Vector Machine, Random Forest, KKNeighborghs, \n",
    "    Complement Naive Bayes, Linear Vector Machine\n",
    "    '''\n",
    "    models=[nby1,svc,rf1,KKN,Lsvc]\n",
    "    # List all the grids created\n",
    "    \n",
    "    names=[\"NaiveBayes\",\"SVC\",\"RandomF\",\"KNeigbhbor\",\"Linear_SVC\"] \n",
    "    # Names of the models created\n",
    "\n",
    "    for model in models:\n",
    "        model.fit(X_train_texto,y_train)\n",
    "        pred = model.predict(X_test_texto)\n",
    "        print(metrics.confusion_matrix(y_test,pred))\n",
    "        #fpr , tpr, thresholds = roc_curve(y_test, pred)\n",
    "        #area=metrics.auc(fpr,tpr)\n",
    "        accuracy=metrics.accuracy_score(y_test, pred)\n",
    "        scores.loc[models.index(model)]=[Country,names[models.index(model)],accuracy]\n",
    "#       fits.loc[models.index(model)]=[Country,names[models.index(model)],model,\n",
    "#                                       model.n_splits_*len(model.cv_results_[\"mean_fit_time\"]),\n",
    "#                                       model.n_splits_] # Not working\n",
    "#\n",
    "#  Pending: \n",
    "#  Get the fpr, tpr, and thresholds and the number of fits by model\n",
    "#  add the models as arguments\n",
    "\n",
    "\n",
    "def estimation2(X_train_texto, y_train, X_test_texto, y_test, Country):\n",
    "    '''\n",
    "    Estimates: Naive bayes, Logit, Vector Machine, Random Forest, KKNeighborghs, Complement Naive Bayes, \n",
    "    Linear Vector Machine\n",
    "    '''\n",
    "    models=[nby1,Lsvc]\n",
    "    names=[\"nby1\",\"Linear_SVC\"] \n",
    "    for model in models:\n",
    "        model.fit(X_train_texto,y_train)\n",
    "        pred = model.predict(X_test_texto)\n",
    "        print(metrics.confusion_matrix(y_test,pred))\n",
    "        #fpr , tpr, thresholds = roc_curve(y_test, pred)\n",
    "        #area=metrics.auc(fpr,tpr)\n",
    "        accuracy=metrics.accuracy_score(y_test, pred)\n",
    "        scores.loc[models.index(model)]=[Country,names[models.index(model)],accuracy]\n",
    "#       fits.loc[models.index(model)]=[Country,names[models.index(model)],model,\n",
    "#                                       model.n_splits_*len(model.cv_results_[\"mean_fit_time\"]),\n",
    "#                                       model.n_splits_] # Not working\n",
    "#\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17ef0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
